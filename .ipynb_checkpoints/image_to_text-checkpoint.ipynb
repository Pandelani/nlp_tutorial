{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a representation from image to text\n",
    "\n",
    "In this notebook we will be using neural networks to learn a representation from an image to text. In this specific instance, we will be getting an image, generating neural features using a deep convolutional neural network trained on image net, and predicting words in a sentance, word by word, not dissimilar to the process of generating characters using the infamous char-rnn.\n",
    "\n",
    "[Original code from GitHub.](https://github.com/anuragmishracse/caption_generator/) This is a modified version that hopefully simplifies, explains and contains all the functionality in one notebook. First lets import Keras and the other modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import SVG\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "We will be using the Flickr8k dataset for the training of this model. This is a dataset of around eight thousand images, all with around three to five human authored captions describing the image. \n",
    "\n",
    "### Download the data\n",
    "\n",
    "Fill out this form and download the data from [this link](https://forms.illinois.edu/sec/1713398). Unzip and move the downloaded folders to the folder that contains this notebook.\n",
    "\n",
    "### Create the dataset\n",
    "\n",
    "Before you continue, ensure that the folder with images is in the folder that contains this notebook, and is called `Flicker8k_Dataset`. Also ensure that the folder that contains all of the flicker .txt files is also in the encapsulating folder for this notebook, and the text folder is titled `Flickr8k_text`. Otherwise, rename the paths used in the code appropriately to where the data is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalise_text(text):\n",
    "    \"\"\"Remove all punctution and return the text as lowercase.\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is open the text files that have the image ids for the the train and test datasets. We read the files, split the lines and store the ids in the `train_images` and `test_images` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images_path = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_images_path  = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "captions_path     = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "\n",
    "# Get the training image names.\n",
    "with open(train_images_path) as file_pointer:\n",
    "    train_images = file_pointer.read().strip('\\n').split()\n",
    "\n",
    "# Get the testing image names.\n",
    "with open(test_images_path) as file_pointer:\n",
    "    test_images = file_pointer.read().strip('\\n').split()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the captions\n",
    "\n",
    "We now need to get the captions. We read the captions and add each caption to a list stored in a dictionary at the key of the respective image ID. We also construct word to token _and_ token to word dicts, to that we can go from words, to embeddings, and back again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words (including tokens): 8831\n",
      "Max caption length: 38\n"
     ]
    }
   ],
   "source": [
    "start_word = '<START>'\n",
    "pad_word   = '<PAD>'\n",
    "end_word   = '<END>'\n",
    "\n",
    "# Get the captions into a dictionary containing lists \n",
    "# of captions.\n",
    "with open(captions_path) as file_pointer:\n",
    "    \n",
    "    image_captions_dict = dict()\n",
    "    max_caption_length = 0\n",
    "    words_to_token_dict = dict()\n",
    "    words_to_token_dict[pad_word] = 0\n",
    "    words_to_token_dict[start_word] = 1\n",
    "    words_to_token_dict[end_word] = 2\n",
    "    word_counter = 3\n",
    "    \n",
    "    # Get massive string.\n",
    "    captions = file_pointer.read().strip().split('\\n')\n",
    "    \n",
    "    # Each row is a sentance and image ID.\n",
    "    for row in captions:\n",
    "        \n",
    "        # Seperate the id and caption.\n",
    "        row = row.split('\\t')\n",
    "        image_id = row[0][:-2]\n",
    "        text = row[1]\n",
    "        \n",
    "        # Remove caption punctuation and make lower case.\n",
    "        text = normalise_text(text)\n",
    "        \n",
    "        # Split caption into words.\n",
    "        text = text.split()\n",
    "        \n",
    "        # Add special tokens.\n",
    "        text = [start_word] + text + [end_word]\n",
    "        \n",
    "        caption_length = len(text)\n",
    "        \n",
    "        # Try appending to list for image id else create a \n",
    "        # list.\n",
    "        try:\n",
    "            image_captions_dict[image_id].append(text)\n",
    "        except:\n",
    "            image_captions_dict[image_id] = [text]\n",
    "            \n",
    "        # We will need to know the longest caption length.\n",
    "        if caption_length > max_caption_length:\n",
    "            max_caption_length = caption_length\n",
    "            \n",
    "        # Add words to dictionary.\n",
    "        for word in text:\n",
    "            if not word in words_to_token_dict:\n",
    "                words_to_token_dict[word] = word_counter\n",
    "                word_counter += 1\n",
    "\n",
    "# Create the inverse dict.                \n",
    "token_to_word_dict = dict()\n",
    "for key, value in words_to_token_dict.items():\n",
    "    token_to_word_dict[value] = key\n",
    "\n",
    "print(\"Unique words (including tokens):\", len(token_to_word_dict))\n",
    "print(\"Max caption length:\", max_caption_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to make the dataset into a list that can be shuffled. We need to pad the sentances that aren't the same length as max_caption_length. We just loop through the captions and prepend pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(image_ids, \n",
    "                image_captions_dict, \n",
    "                max_caption_length,\n",
    "                words_to_token_dict):\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Loop over all of the image ids in the dataset.\n",
    "    for image_id in image_ids:\n",
    "        \n",
    "        captions = image_captions_dict[image_id]\n",
    "        \n",
    "        # Loop over every caption for this given image id.\n",
    "        for caption in captions:\n",
    "            \n",
    "            # Loop over the length of the words in the caption.\n",
    "            for i in range(1, len(caption) - 1):\n",
    "                \n",
    "                # Get the preceeding words.\n",
    "                previous_words = caption[:i]\n",
    "                \n",
    "                # How many do we need to pad this by?\n",
    "                pad_depth = max_caption_length - len(previous_words)\n",
    "        \n",
    "                # The pad word list to be prepended.\n",
    "                pad_words = [pad_word for _ in range(pad_depth)]\n",
    "                \n",
    "                # The padded preceeding words.\n",
    "                previous_words = pad_words + previous_words\n",
    "                \n",
    "                assert len(previous_words) == max_caption_length\n",
    "                \n",
    "                next_word = caption[i]\n",
    "                \n",
    "                # Tokenise.\n",
    "                previous_tokens = [words_to_token_dict[word]\n",
    "                                   for word in previous_words]\n",
    "                next_token = words_to_token_dict[next_word]\n",
    "                \n",
    "                data_point = (previous_tokens, image_id, next_token)\n",
    "                dataset.append(data_point)\n",
    "                \n",
    "    return dataset\n",
    "                \n",
    "    \n",
    "train_set = get_dataset(train_images, \n",
    "                        image_captions_dict, \n",
    "                        max_caption_length, \n",
    "                        words_to_token_dict)\n",
    "test_set  = get_dataset(test_images, \n",
    "                        image_captions_dict, \n",
    "                        max_caption_length, \n",
    "                        words_to_token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length test dataset:  54208\n",
      "length train dataset: 323639\n"
     ]
    }
   ],
   "source": [
    "print(\"length test dataset: \", len(test_set))\n",
    "print(\"length train dataset:\", len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words = len(token_to_word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "This is one of the more interesting parts of this notebook; we define the neural network here; the neural network is comprised of three mini-neural networks, all focusing on slightly different tasks, yet the model is entirely joined and fully differentiable!\n",
    "\n",
    "Three points of interest:\n",
    "- The image encoder: _A deep neural network (VGG16) trained on image net is used as a image feature extractor. The softmax activation is chopped off, so the model takes 150528 pixel values - a shape of (224, 224, 3) - and outputs 1000 values as a vector of which classes it thinks are present in the image._\n",
    "- The sentance encoder: _A recurrent neural network that takes embeddings of tokens comprising a sentance that learns some representation of the sentance. The RNN outputs a sequence, and at each timestep, a one layer MLP is applied to each output of the RNN._\n",
    "- The main model: _A recurrent neural network that takes the concatenation of the image encoder and the sentance encoder, and tries to predict the next word out of every possible word in the dataset (around 8000)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model(unique_words,\n",
    "              minimise_image_features=-1, \n",
    "              train_image_encoder=False, \n",
    "              embedding_size=256,\n",
    "              langauge_lstm_size=256,\n",
    "              langauge_output_size=128,\n",
    "              main_lstm_size=1000):\n",
    "    # Add the feature extractor - Could try Inception/MobileNet\n",
    "    # with trainable weights!\n",
    "    base_model = VGG16(weights='imagenet', \n",
    "                       include_top=True, \n",
    "                       input_shape=(224, 224, 3))\n",
    "\n",
    "    # Get rid of the classification layer.\n",
    "    base_model.layers.pop() \n",
    "    base_model.outputs = [base_model.layers[-1].output]\n",
    "    base_model.layers[-1].outbound_nodes = []\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = train_image_encoder\n",
    "\n",
    "    # The image encoder.\n",
    "    image_model = Sequential()\n",
    "    image_model.add(base_model)\n",
    "    if minimise_image_features is not -1:\n",
    "        image_model.add(Dense(embedding_size))\n",
    "    image_model.add(RepeatVector(max_caption_length))\n",
    "\n",
    "    # The sentance encoder.\n",
    "    language_model = Sequential()\n",
    "    language_model.add(Embedding(unique_words,\n",
    "                                 embedding_size, \n",
    "                                 input_length=max_caption_length))\n",
    "    language_model.add(LSTM(langauge_lstm_size, return_sequences=True))\n",
    "    language_model.add(TimeDistributed(Dense(langauge_output_size)))\n",
    "\n",
    "    # The main model.\n",
    "    model = Sequential()\n",
    "    model.add(Merge([image_model, language_model], mode='concat'))\n",
    "    model.add(LSTM(main_lstm_size, return_sequences=False))\n",
    "    model.add(Dense(unique_words))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model (with multiple GPU support)\n",
    "\n",
    "Keras makes multiple GPU support pretty painless. Essentially it just copies the model onto each GPU, and divides the batch size by the amount of GPU's you have, and concatenates each model's mini batch for the loss function. \n",
    "\n",
    "If you have more than one GPU available, be sure to set the `amount_gpus` to the appropriate value! Otherwise ensure it is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"636pt\" viewBox=\"0.00 0.00 848.00 636.00\" width=\"848pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 632)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-632 844,-632 844,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139834720841968 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139834720841968</title>\n",
       "<polygon fill=\"none\" points=\"74,-581.5 74,-627.5 398,-627.5 398,-581.5 74,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-600.8\">embedding_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"267,-581.5 267,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"267,-604.5 322,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"322,-581.5 322,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360\" y=\"-612.3\">(None, 38)</text>\n",
       "<polyline fill=\"none\" points=\"322,-604.5 398,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360\" y=\"-589.3\">(None, 38)</text>\n",
       "</g>\n",
       "<!-- 139834720842920 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139834720842920</title>\n",
       "<polygon fill=\"none\" points=\"76,-498.5 76,-544.5 396,-544.5 396,-498.5 76,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156.5\" y=\"-517.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"237,-498.5 237,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"237,-521.5 292,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"292,-498.5 292,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"344\" y=\"-529.3\">(None, 38)</text>\n",
       "<polyline fill=\"none\" points=\"292,-521.5 396,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"344\" y=\"-506.3\">(None, 38, 256)</text>\n",
       "</g>\n",
       "<!-- 139834720841968&#45;&gt;139834720842920 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139834720841968-&gt;139834720842920</title>\n",
       "<path d=\"M236,-581.366C236,-573.152 236,-563.658 236,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"239.5,-554.607 236,-544.607 232.5,-554.607 239.5,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139834720843256 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139834720843256</title>\n",
       "<polygon fill=\"none\" points=\"499,-498.5 499,-544.5 831,-544.5 831,-498.5 499,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"575\" y=\"-517.8\">vgg16_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"651,-498.5 651,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"678.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"651,-521.5 706,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"678.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"706,-498.5 706,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"768.5\" y=\"-529.3\">(None, 224, 224, 3)</text>\n",
       "<polyline fill=\"none\" points=\"706,-521.5 831,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"768.5\" y=\"-506.3\">(None, 224, 224, 3)</text>\n",
       "</g>\n",
       "<!-- 139834721556184 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139834721556184</title>\n",
       "<polygon fill=\"none\" points=\"528,-415.5 528,-461.5 802,-461.5 802,-415.5 528,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"575\" y=\"-434.8\">vgg16: Model</text>\n",
       "<polyline fill=\"none\" points=\"622,-415.5 622,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"649.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"622,-438.5 677,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"649.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"677,-415.5 677,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"739.5\" y=\"-446.3\">(None, 224, 224, 3)</text>\n",
       "<polyline fill=\"none\" points=\"677,-438.5 802,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"739.5\" y=\"-423.3\">multiple</text>\n",
       "</g>\n",
       "<!-- 139834720843256&#45;&gt;139834721556184 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139834720843256-&gt;139834721556184</title>\n",
       "<path d=\"M665,-498.366C665,-490.152 665,-480.658 665,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"668.5,-471.607 665,-461.607 661.5,-471.607 668.5,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139833935273880 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139833935273880</title>\n",
       "<polygon fill=\"none\" points=\"107.5,-415.5 107.5,-461.5 364.5,-461.5 364.5,-415.5 107.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156.5\" y=\"-434.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"205.5,-415.5 205.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"205.5,-438.5 260.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"233\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"260.5,-415.5 260.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"312.5\" y=\"-446.3\">(None, 38, 256)</text>\n",
       "<polyline fill=\"none\" points=\"260.5,-438.5 364.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"312.5\" y=\"-423.3\">(None, 38, 256)</text>\n",
       "</g>\n",
       "<!-- 139834720842920&#45;&gt;139833935273880 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139834720842920-&gt;139833935273880</title>\n",
       "<path d=\"M236,-498.366C236,-490.152 236,-480.658 236,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"239.5,-471.607 236,-461.607 232.5,-471.607 239.5,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139833935322192 -->\n",
       "<g class=\"node\" id=\"node6\"><title>139833935322192</title>\n",
       "<polygon fill=\"none\" points=\"490,-332.5 490,-378.5 840,-378.5 840,-332.5 490,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"582.5\" y=\"-351.8\">repeat_vector_1: RepeatVector</text>\n",
       "<polyline fill=\"none\" points=\"675,-332.5 675,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"702.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"675,-355.5 730,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"702.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"730,-332.5 730,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"785\" y=\"-363.3\">(None, 4096)</text>\n",
       "<polyline fill=\"none\" points=\"730,-355.5 840,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"785\" y=\"-340.3\">(None, 38, 4096)</text>\n",
       "</g>\n",
       "<!-- 139834721556184&#45;&gt;139833935322192 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139834721556184-&gt;139833935322192</title>\n",
       "<path d=\"M665,-415.366C665,-407.152 665,-397.658 665,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"668.5,-388.607 665,-378.607 661.5,-388.607 668.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139834677767920 -->\n",
       "<g class=\"node\" id=\"node7\"><title>139834677767920</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 472,-378.5 472,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156.5\" y=\"-351.8\">time_distributed_1(dense_1): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"313,-332.5 313,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"340.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"313,-355.5 368,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"340.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"368,-332.5 368,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420\" y=\"-363.3\">(None, 38, 256)</text>\n",
       "<polyline fill=\"none\" points=\"368,-355.5 472,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420\" y=\"-340.3\">(None, 38, 128)</text>\n",
       "</g>\n",
       "<!-- 139833935273880&#45;&gt;139834677767920 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>139833935273880-&gt;139834677767920</title>\n",
       "<path d=\"M236,-415.366C236,-407.152 236,-397.658 236,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"239.5,-388.607 236,-378.607 232.5,-388.607 239.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139834677767080 -->\n",
       "<g class=\"node\" id=\"node8\"><title>139834677767080</title>\n",
       "<polygon fill=\"none\" points=\"261.5,-249.5 261.5,-295.5 638.5,-295.5 638.5,-249.5 261.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315.5\" y=\"-268.8\">merge_1: Merge</text>\n",
       "<polyline fill=\"none\" points=\"369.5,-249.5 369.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"369.5,-272.5 424.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"424.5,-249.5 424.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531.5\" y=\"-280.3\">[(None, 38, 4096), (None, 38, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"424.5,-272.5 638.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"531.5\" y=\"-257.3\">(None, 38, 4224)</text>\n",
       "</g>\n",
       "<!-- 139833935322192&#45;&gt;139834677767080 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>139833935322192-&gt;139834677767080</title>\n",
       "<path d=\"M606.534,-332.473C579.163,-322.162 546.395,-309.816 517.929,-299.092\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"519.046,-295.773 508.454,-295.522 516.578,-302.323 519.046,-295.773\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139834677767920&#45;&gt;139834677767080 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>139834677767920-&gt;139834677767080</title>\n",
       "<path d=\"M294.194,-332.473C321.437,-322.162 354.053,-309.816 382.387,-299.092\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"383.704,-302.336 391.818,-295.522 381.226,-295.789 383.704,-302.336\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139834676403168 -->\n",
       "<g class=\"node\" id=\"node9\"><title>139834676403168</title>\n",
       "<polygon fill=\"none\" points=\"318.5,-166.5 318.5,-212.5 581.5,-212.5 581.5,-166.5 318.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"367.5\" y=\"-185.8\">lstm_2: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"416.5,-166.5 416.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"416.5,-189.5 471.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"471.5,-166.5 471.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"526.5\" y=\"-197.3\">(None, 38, 4224)</text>\n",
       "<polyline fill=\"none\" points=\"471.5,-189.5 581.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"526.5\" y=\"-174.3\">(None, 1000)</text>\n",
       "</g>\n",
       "<!-- 139834677767080&#45;&gt;139834676403168 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>139834677767080-&gt;139834676403168</title>\n",
       "<path d=\"M450,-249.366C450,-241.152 450,-231.658 450,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"453.5,-222.607 450,-212.607 446.5,-222.607 453.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139834675971352 -->\n",
       "<g class=\"node\" id=\"node10\"><title>139834675971352</title>\n",
       "<polygon fill=\"none\" points=\"327,-83.5 327,-129.5 573,-129.5 573,-83.5 327,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"378\" y=\"-102.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"429,-83.5 429,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"456.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"429,-106.5 484,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"456.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"484,-83.5 484,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"528.5\" y=\"-114.3\">(None, 1000)</text>\n",
       "<polyline fill=\"none\" points=\"484,-106.5 573,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"528.5\" y=\"-91.3\">(None, 8831)</text>\n",
       "</g>\n",
       "<!-- 139834676403168&#45;&gt;139834675971352 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>139834676403168-&gt;139834675971352</title>\n",
       "<path d=\"M450,-166.366C450,-158.152 450,-148.658 450,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"453.5,-139.607 450,-129.607 446.5,-139.607 453.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139834658646672 -->\n",
       "<g class=\"node\" id=\"node11\"><title>139834658646672</title>\n",
       "<polygon fill=\"none\" points=\"304,-0.5 304,-46.5 596,-46.5 596,-0.5 304,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"378\" y=\"-19.8\">activation_1: Activation</text>\n",
       "<polyline fill=\"none\" points=\"452,-0.5 452,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"452,-23.5 507,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"479.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"507,-0.5 507,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"551.5\" y=\"-31.3\">(None, 8831)</text>\n",
       "<polyline fill=\"none\" points=\"507,-23.5 596,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"551.5\" y=\"-8.3\">(None, 8831)</text>\n",
       "</g>\n",
       "<!-- 139834675971352&#45;&gt;139834658646672 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>139834675971352-&gt;139834658646672</title>\n",
       "<path d=\"M450,-83.3664C450,-75.1516 450,-65.6579 450,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"453.5,-56.6068 450,-46.6068 446.5,-56.6069 453.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount_gpus = 1\n",
    "\n",
    "if amount_gpus > 1:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = get_model(unique_words)\n",
    "        model = multi_gpu_model(model, gpus=2)\n",
    "else:\n",
    "    model = get_model(unique_words)\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='SGD', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data\n",
    "\n",
    "This is just a fancy way of yeilding data just like a Python list comprehension would generate a list. We pass the class the dataset we made earlier, and it will create batches of unifinished sentances, batches of the respective next words, batches of image ids, load the batch of image ids as images into memory and yeild them ultimately to the Keras model when `data_generator` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    path = os.path.join('./Flicker8k_Dataset', path)\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "class DataGenerator():\n",
    "    \n",
    "    \n",
    "    def __init__(self, partial_sentances, images, next_words, vocab_size):\n",
    "        self.images = np.array(images)\n",
    "        self.partial_sentances = np.array(partial_sentances) \n",
    "        self.next_words = np.array(next_words)\n",
    "        assert len(self.images) == len(self.partial_sentances)\n",
    "        assert len(self.next_words) == len(self.images)\n",
    "        self.data_size = len(self.images)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        \n",
    "    def data_generator(self, batch_size, shuffle=True):\n",
    "        data_size = self.data_size\n",
    "        while True:\n",
    "            \n",
    "#             if shuffle:\n",
    "#                 permutation = np.random.permutation(data_size)\n",
    "#                 self.images = self.images[permutation]\n",
    "#                 self.partial_sentances = self.partial_sentances[permutation]\n",
    "#                 self.next_words = self.next_words[permutation]\n",
    "            \n",
    "            for start in range(data_size - batch_size):\n",
    "                end = start + batch_size\n",
    "                \n",
    "                batch_image_paths = self.images[start:end]\n",
    "                batch_next_words = self.next_words[start:end]\n",
    "                batch_partial_sentances = self.partial_sentances[start:end]\n",
    "                \n",
    "                batch_images = np.array([load_image(image) \n",
    "                                         for image in batch_image_paths])\n",
    "                batch_x = [batch_images, batch_partial_sentances]\n",
    "                \n",
    "                batch_y = to_categorical(batch_next_words)\n",
    "                    \n",
    "                yield [batch_x, batch_y]\n",
    "                \n",
    "                \n",
    "partial_sentances, images, next_words = zip(*train_set)\n",
    "data_gen = DataGenerator(partial_sentances, \n",
    "                         images, \n",
    "                         next_words, \n",
    "                         unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "_This will take some time._ \n",
    "\n",
    "We define a checkpoint that saves the weights if the loss is improved. This is a form of early stopping. Go find something else to do for the day (or week depending on the quality of your graphics card) and leave this running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "file_name = 'weights-improvement-{epoch:02d}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5057/5056 [==============================] - 2508s 496ms/step - loss: 3.8362 - acc: 0.1555\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.83623, saving model to weights-improvement-01.hdf5\n",
      "Epoch 2/50\n",
      "5057/5056 [==============================] - 2507s 496ms/step - loss: 3.6750 - acc: 0.1535\n",
      "\n",
      "Epoch 00002: loss improved from 3.83623 to 3.67496, saving model to weights-improvement-02.hdf5\n",
      "Epoch 3/50\n",
      "5057/5056 [==============================] - 2507s 496ms/step - loss: 3.6456 - acc: 0.1560\n",
      "\n",
      "Epoch 00003: loss improved from 3.67496 to 3.64556, saving model to weights-improvement-03.hdf5\n",
      "Epoch 4/50\n",
      "5057/5056 [==============================] - 2507s 496ms/step - loss: 3.6502 - acc: 0.1566\n",
      "\n",
      "Epoch 00004: loss did not improve\n",
      "Epoch 5/50\n",
      "5057/5056 [==============================] - 2505s 495ms/step - loss: 3.6623 - acc: 0.1603\n",
      "\n",
      "Epoch 00005: loss did not improve\n",
      "Epoch 6/50\n",
      "5057/5056 [==============================] - 2504s 495ms/step - loss: 3.7085 - acc: 0.1557\n",
      "\n",
      "Epoch 00006: loss did not improve\n",
      "Epoch 7/50\n",
      " 365/5056 [=>............................] - ETA: 38:43 - loss: 3.5659 - acc: 0.1682"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(file_name, \n",
    "                             monitor='loss',\n",
    "                             verbose=1, \n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "model.fit_generator(data_gen.data_generator(batch_size),\n",
    "                    steps_per_epoch=data_gen.data_size / batch_size, \n",
    "                    epochs=epochs, \n",
    "                    verbose=1, \n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./weights-improvement-01.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
