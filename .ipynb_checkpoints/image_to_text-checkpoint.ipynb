{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning a representation from image to text\n",
    "\n",
    "In this notebook we will be using neural networks to learn a representation from an image to text. In this specific instance, we will be getting an image, generating neural features using a deep convolutional neural network trained on image net, and predicting words in a sentance, word by word, not dissimilar to the process of generating characters using the infamous char-rnn.\n",
    "\n",
    "[Original code from GitHub.](https://github.com/anuragmishracse/caption_generator/) This is a modified version that hopefully simplifies, explains and contains all the functionality in one notebook. First lets import Keras and the other modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eavi/leon/keras_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import SVG\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Merge, Activation, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "We will be using the Flickr8k dataset for the training of this model. This is a dataset of around eight thousand images, all with around three to five human authored captions describing the image. \n",
    "\n",
    "### Download the data\n",
    "\n",
    "Fill out this form and download the data from [this link](https://forms.illinois.edu/sec/1713398). Unzip and move the downloaded folders to the folder that contains this notebook.\n",
    "\n",
    "### Create the dataset\n",
    "\n",
    "Before you continue, ensure that the folder with images is in the folder that contains this notebook, and is called `Flicker8k_Dataset`. Also ensure that the folder that contains all of the flicker .txt files is also in the encapsulating folder for this notebook, and the text folder is titled `Flickr8k_text`. Otherwise, rename the paths used in the code appropriately to where the data is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_text(text):\n",
    "    \"\"\"Remove all punctution and return the text as lowercase.\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is open the text files that have the image ids for the the train and test datasets. We read the files, split the lines and store the ids in the `train_images` and `test_images` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "test_images_path  = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "captions_path     = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "\n",
    "# Get the training image names.\n",
    "with open(train_images_path) as file_pointer:\n",
    "    train_images = file_pointer.read().strip('\\n').split()\n",
    "\n",
    "# Get the testing image names.\n",
    "with open(test_images_path) as file_pointer:\n",
    "    test_images = file_pointer.read().strip('\\n').split()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the captions\n",
    "\n",
    "We now need to get the captions. We read the captions and add each caption to a list stored in a dictionary at the key of the respective image ID. We also construct word to token _and_ token to word dicts, to that we can go from words, to embeddings, and back again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words (including tokens): 8831\n",
      "Max caption length: 38\n"
     ]
    }
   ],
   "source": [
    "start_word = '<START>'\n",
    "pad_word   = '<PAD>'\n",
    "end_word   = '<END>'\n",
    "\n",
    "# Get the captions into a dictionary containing lists \n",
    "# of captions.\n",
    "with open(captions_path) as file_pointer:\n",
    "    \n",
    "    image_captions_dict = dict()\n",
    "    max_caption_length = 0\n",
    "    words_to_token_dict = dict()\n",
    "    words_to_token_dict[pad_word] = 0\n",
    "    words_to_token_dict[start_word] = 1\n",
    "    words_to_token_dict[end_word] = 2\n",
    "    word_counter = 3\n",
    "    \n",
    "    # Get massive string.\n",
    "    captions = file_pointer.read().strip().split('\\n')\n",
    "    \n",
    "    # Each row is a sentance and image ID.\n",
    "    for row in captions:\n",
    "        \n",
    "        # Seperate the id and caption.\n",
    "        row = row.split('\\t')\n",
    "        image_id = row[0][:-2]\n",
    "        text = row[1]\n",
    "        \n",
    "        # Remove caption punctuation and make lower case.\n",
    "        text = normalise_text(text)\n",
    "        \n",
    "        # Split caption into words.\n",
    "        text = text.split()\n",
    "        \n",
    "        # Add special tokens.\n",
    "        text = [start_word] + text + [end_word]\n",
    "        \n",
    "        caption_length = len(text)\n",
    "        \n",
    "        # Try appending to list for image id else create a \n",
    "        # list.\n",
    "        try:\n",
    "            image_captions_dict[image_id].append(text)\n",
    "        except:\n",
    "            image_captions_dict[image_id] = [text]\n",
    "            \n",
    "        # We will need to know the longest caption length.\n",
    "        if caption_length > max_caption_length:\n",
    "            max_caption_length = caption_length\n",
    "            \n",
    "        # Add words to dictionary.\n",
    "        for word in text:\n",
    "            if not word in words_to_token_dict:\n",
    "                words_to_token_dict[word] = word_counter\n",
    "                word_counter += 1\n",
    "\n",
    "# Create the inverse dict.                \n",
    "token_to_word_dict = dict()\n",
    "for key, value in words_to_token_dict.items():\n",
    "    token_to_word_dict[value] = key\n",
    "\n",
    "print(\"Unique words (including tokens):\", len(token_to_word_dict))\n",
    "print(\"Max caption length:\", max_caption_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to make the dataset into a list that can be shuffled. We need to pad the sentances that aren't the same length as max_caption_length. We just loop through the captions and prepend pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(image_ids, \n",
    "                image_captions_dict, \n",
    "                max_caption_length,\n",
    "                words_to_token_dict):\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Loop over all of the image ids in the dataset.\n",
    "    for image_id in image_ids:\n",
    "        \n",
    "        captions = image_captions_dict[image_id]\n",
    "        \n",
    "        # Loop over every caption for this given image id.\n",
    "        for caption in captions:\n",
    "            \n",
    "            # Loop over the length of the words in the caption.\n",
    "            for i in range(1, len(caption) - 1):\n",
    "                \n",
    "                # Get the preceeding words.\n",
    "                previous_words = caption[:i]\n",
    "                \n",
    "                # How many do we need to pad this by?\n",
    "                pad_depth = max_caption_length - len(previous_words)\n",
    "        \n",
    "                # The pad word list to be prepended.\n",
    "                pad_words = [pad_word for _ in range(pad_depth)]\n",
    "                \n",
    "                # The padded preceeding words.\n",
    "                previous_words = pad_words + previous_words\n",
    "                \n",
    "                assert len(previous_words) == max_caption_length\n",
    "                \n",
    "                next_word = caption[i]\n",
    "                \n",
    "                # Tokenise.\n",
    "                previous_tokens = [words_to_token_dict[word]\n",
    "                                   for word in previous_words]\n",
    "                next_token = words_to_token_dict[next_word]\n",
    "                \n",
    "                data_point = (previous_tokens, image_id, next_token)\n",
    "                dataset.append(data_point)\n",
    "                \n",
    "    return dataset\n",
    "                \n",
    "    \n",
    "train_set = get_dataset(train_images, \n",
    "                        image_captions_dict, \n",
    "                        max_caption_length, \n",
    "                        words_to_token_dict)\n",
    "test_set  = get_dataset(test_images, \n",
    "                        image_captions_dict, \n",
    "                        max_caption_length, \n",
    "                        words_to_token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length test dataset:  54208\n",
      "length train dataset: 323639\n"
     ]
    }
   ],
   "source": [
    "print(\"length test dataset: \", len(test_set))\n",
    "print(\"length train dataset:\", len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = len(token_to_word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "This is one of the more interesting parts of this notebook; we define the neural network here; the neural network is comprised of three mini-neural networks, all focusing on slightly different tasks, yet the model is entirely joined and fully differentiable!\n",
    "\n",
    "Three points of interest:\n",
    "- The image encoder: _A deep neural network (VGG16) trained on image net is used as a image feature extractor. The softmax activation is chopped off, so the model takes 150528 pixel values - a shape of (224, 224, 3) - and outputs 1000 values as a vector of which classes it thinks are present in the image._\n",
    "- The sentance encoder: _A recurrent neural network that takes embeddings of tokens comprising a sentance that learns some representation of the sentance. The RNN outputs a sequence, and at each timestep, a one layer MLP is applied to each output of the RNN._\n",
    "- The main model: _A recurrent neural network that takes the concatenation of the image encoder and the sentance encoder, and tries to predict the next word out of every possible word in the dataset (around 8000)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"304pt\" viewBox=\"0.00 0.00 1176.50 304.00\" width=\"1177pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 1172.5,-300 1172.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140488700522112 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140488700522112</title>\n",
       "<polygon fill=\"none\" points=\"213.5,-249.5 213.5,-295.5 545.5,-295.5 545.5,-249.5 213.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"289.5\" y=\"-268.8\">vgg16_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"365.5,-249.5 365.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"393\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"365.5,-272.5 420.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"393\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"420.5,-249.5 420.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"483\" y=\"-280.3\">(None, 224, 224, 3)</text>\n",
       "<polyline fill=\"none\" points=\"420.5,-272.5 545.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"483\" y=\"-257.3\">(None, 224, 224, 3)</text>\n",
       "</g>\n",
       "<!-- 140488639294376 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140488639294376</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 303,-212.5 303,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"61.5\" y=\"-185.8\">lambda_1: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"123,-166.5 123,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"150.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"123,-189.5 178,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"150.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"178,-166.5 178,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.5\" y=\"-197.3\">(None, 224, 224, 3)</text>\n",
       "<polyline fill=\"none\" points=\"178,-189.5 303,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240.5\" y=\"-174.3\">(None, 224, 224, 3)</text>\n",
       "</g>\n",
       "<!-- 140488700522112&#45;&gt;140488639294376 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140488700522112-&gt;140488639294376</title>\n",
       "<path d=\"M317.499,-249.473C288.347,-239.117 253.421,-226.709 223.143,-215.952\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.083,-212.572 213.488,-212.522 221.74,-219.168 224.083,-212.572\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488639295440 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140488639295440</title>\n",
       "<polygon fill=\"none\" points=\"321,-166.5 321,-212.5 624,-212.5 624,-166.5 321,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"382.5\" y=\"-185.8\">lambda_3: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"444,-166.5 444,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"471.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"444,-189.5 499,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"471.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"499,-166.5 499,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-197.3\">(None, 224, 224, 3)</text>\n",
       "<polyline fill=\"none\" points=\"499,-189.5 624,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561.5\" y=\"-174.3\">(None, 224, 224, 3)</text>\n",
       "</g>\n",
       "<!-- 140488700522112&#45;&gt;140488639295440 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140488700522112-&gt;140488639295440</title>\n",
       "<path d=\"M404.913,-249.366C415.575,-240.08 428.113,-229.16 439.476,-219.262\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"441.876,-221.814 447.118,-212.607 437.278,-216.535 441.876,-221.814\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488699514216 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140488699514216</title>\n",
       "<polygon fill=\"none\" points=\"699.5,-249.5 699.5,-295.5 1023.5,-295.5 1023.5,-249.5 699.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"796\" y=\"-268.8\">embedding_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"892.5,-249.5 892.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"920\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"892.5,-272.5 947.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"920\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"947.5,-249.5 947.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"985.5\" y=\"-280.3\">(None, 38)</text>\n",
       "<polyline fill=\"none\" points=\"947.5,-272.5 1023.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"985.5\" y=\"-257.3\">(None, 38)</text>\n",
       "</g>\n",
       "<!-- 140488639292640 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140488639292640</title>\n",
       "<polygon fill=\"none\" points=\"642.5,-166.5 642.5,-212.5 896.5,-212.5 896.5,-166.5 642.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"704\" y=\"-185.8\">lambda_2: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"765.5,-166.5 765.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"793\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"765.5,-189.5 820.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"793\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"820.5,-166.5 820.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"858.5\" y=\"-197.3\">(None, 38)</text>\n",
       "<polyline fill=\"none\" points=\"820.5,-189.5 896.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"858.5\" y=\"-174.3\">(None, 38)</text>\n",
       "</g>\n",
       "<!-- 140488699514216&#45;&gt;140488639292640 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140488699514216-&gt;140488639292640</title>\n",
       "<path d=\"M836.361,-249.366C825.813,-240.08 813.41,-229.16 802.168,-219.262\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"804.427,-216.588 794.609,-212.607 799.802,-221.842 804.427,-216.588\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488638656296 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140488638656296</title>\n",
       "<polygon fill=\"none\" points=\"914.5,-166.5 914.5,-212.5 1168.5,-212.5 1168.5,-166.5 914.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"976\" y=\"-185.8\">lambda_4: Lambda</text>\n",
       "<polyline fill=\"none\" points=\"1037.5,-166.5 1037.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1065\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1037.5,-189.5 1092.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1065\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1092.5,-166.5 1092.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-197.3\">(None, 38)</text>\n",
       "<polyline fill=\"none\" points=\"1092.5,-189.5 1168.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1130.5\" y=\"-174.3\">(None, 38)</text>\n",
       "</g>\n",
       "<!-- 140488699514216&#45;&gt;140488638656296 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140488699514216-&gt;140488638656296</title>\n",
       "<path d=\"M910.686,-249.366C933.108,-239.277 959.817,-227.257 983.238,-216.718\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"984.691,-219.902 992.374,-212.607 981.818,-213.519 984.691,-219.902\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488658717664 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140488658717664</title>\n",
       "<polygon fill=\"none\" points=\"418.5,-83.5 418.5,-129.5 822.5,-129.5 822.5,-83.5 418.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"493\" y=\"-102.8\">sequential_3: Sequential</text>\n",
       "<polyline fill=\"none\" points=\"567.5,-83.5 567.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"567.5,-106.5 622.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"622.5,-83.5 622.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"722.5\" y=\"-114.3\">[(None, 224, 224, 3), (None, 38)]</text>\n",
       "<polyline fill=\"none\" points=\"622.5,-106.5 822.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"722.5\" y=\"-91.3\">(None, 8831)</text>\n",
       "</g>\n",
       "<!-- 140488639294376&#45;&gt;140488658717664 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140488639294376-&gt;140488658717664</title>\n",
       "<path d=\"M279.037,-166.473C342.118,-155.579 418.343,-142.414 482.756,-131.289\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"483.73,-134.673 492.989,-129.522 482.539,-127.775 483.73,-134.673\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488639292640&#45;&gt;140488658717664 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140488639292640-&gt;140488658717664</title>\n",
       "<path d=\"M728.785,-166.366C710.635,-156.5 689.092,-144.788 670.018,-134.419\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"671.623,-131.308 661.166,-129.607 668.28,-137.458 671.623,-131.308\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488639295440&#45;&gt;140488658717664 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140488639295440-&gt;140488658717664</title>\n",
       "<path d=\"M512.942,-166.366C530.97,-156.5 552.369,-144.788 571.314,-134.419\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"573.015,-137.478 580.107,-129.607 569.655,-131.338 573.015,-137.478\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488638656296&#45;&gt;140488658717664 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140488638656296-&gt;140488658717664</title>\n",
       "<path d=\"M927.016,-166.473C870.624,-155.624 802.53,-142.522 744.861,-131.427\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"745.442,-127.975 734.961,-129.522 744.12,-134.849 745.442,-127.975\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140488639293368 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140488639293368</title>\n",
       "<polygon fill=\"none\" points=\"424.5,-0.5 424.5,-46.5 816.5,-46.5 816.5,-0.5 424.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"503.5\" y=\"-19.8\">activation_1: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"582.5,-0.5 582.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"610\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"582.5,-23.5 637.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"610\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"637.5,-0.5 637.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"727\" y=\"-31.3\">[(None, 8831), (None, 8831)]</text>\n",
       "<polyline fill=\"none\" points=\"637.5,-23.5 816.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"727\" y=\"-8.3\">(None, 8831)</text>\n",
       "</g>\n",
       "<!-- 140488658717664&#45;&gt;140488639293368 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140488658717664-&gt;140488639293368</title>\n",
       "<path d=\"M620.5,-83.3664C620.5,-75.1516 620.5,-65.6579 620.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"624,-56.6068 620.5,-46.6068 617,-56.6069 624,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model(unique_words,\n",
    "              minimise_image_features=-1, \n",
    "              train_image_encoder=False, \n",
    "              embedding_size=256,\n",
    "              langauge_lstm_size=256,\n",
    "              langauge_output_size=128,\n",
    "              main_lstm_size=1000):\n",
    "    # Add the feature extractor - Could try Inception/MobileNet\n",
    "    # with trainable weights!\n",
    "    base_model = VGG16(weights='imagenet', \n",
    "                       include_top=True, \n",
    "                       input_shape=(224, 224, 3))\n",
    "\n",
    "    # Get rid of the classification layer.\n",
    "    base_model.layers.pop() \n",
    "    base_model.outputs = [base_model.layers[-1].output]\n",
    "    base_model.layers[-1].outbound_nodes = []\n",
    "    base_model.trainable = train_image_encoder\n",
    "\n",
    "    # The image encoder.\n",
    "    image_model = Sequential()\n",
    "    image_model.add(base_model)\n",
    "    if minimise_image_features is not -1:\n",
    "        image_model.add(Dense(embedding_size))\n",
    "    image_model.add(RepeatVector(max_caption_length))\n",
    "\n",
    "    # The sentance encoder.\n",
    "    language_model = Sequential()\n",
    "    language_model.add(Embedding(unique_words,\n",
    "                                 embedding_size, \n",
    "                                 input_length=max_caption_length))\n",
    "    language_model.add(LSTM(langauge_lstm_size, return_sequences=True))\n",
    "    language_model.add(TimeDistributed(Dense(langauge_output_size)))\n",
    "\n",
    "    # The main model.\n",
    "    model = Sequential()\n",
    "    model.add(Merge([image_model, language_model], mode='concat'))\n",
    "    model.add(LSTM(main_lstm_size, return_sequences=False))\n",
    "    model.add(Dense(unique_words))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model (with multiple GPU support)\n",
    "\n",
    "Keras makes multiple GPU support pretty painless. Essentially it just copies the model onto each GPU, and divides the batch size by the amount of GPU's you have, and concatenates each model's mini batch for the loss function. \n",
    "\n",
    "If you have more than one GPU available, be sure to set the `amount_gpus` to the appropriate value! Otherwise ensure it is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_gpus = 2\n",
    "\n",
    "if amount_gpus > 1:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = get_model(unique_words)\n",
    "        model = multi_gpu_model(model, gpus=2)\n",
    "else:\n",
    "    model = get_model(unique_words)\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "SVG(model_to_dot(parallel_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating data\n",
    "\n",
    "This is just a fancy way of yeilding data just like a Python list comprehension would generate a list. We pass the class the dataset we made earlier, and it will create batches of unifinished sentances, batches of the respective next words, batches of image ids, load the batch of image ids as images into memory and yeild them ultimately to the Keras model when `data_generator` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    path = os.path.join('./Flicker8k_Dataset', path)\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "class DataGenerator():\n",
    "    \n",
    "    \n",
    "    def __init__(self, partial_sentances, images, next_words, vocab_size):\n",
    "        self.images = np.array(images)\n",
    "        self.partial_sentances = np.array(partial_sentances) \n",
    "        self.next_words = np.array(next_words)\n",
    "        assert len(self.images) == len(self.partial_sentances)\n",
    "        assert len(self.next_words) == len(self.images)\n",
    "        self.data_size = len(self.images)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        \n",
    "    def data_generator(self, batch_size, shuffle=True):\n",
    "        data_size = self.data_size\n",
    "        while True:\n",
    "            \n",
    "            if shuffle:\n",
    "                permutation = np.random.permutation(data_size)\n",
    "                self.images = self.images[permutation]\n",
    "                self.partial_sentances = self.partial_sentances[permutation]\n",
    "                self.next_words = self.next_words[permutation]\n",
    "            \n",
    "            for start in range(data_size - batch_size):\n",
    "                end = start + batch_size\n",
    "                \n",
    "                batch_image_paths = self.images[start:end]\n",
    "                batch_next_words = self.next_words[start:end]\n",
    "                batch_partial_sentances = self.partial_sentances[start:end]\n",
    "                \n",
    "                batch_images = np.array([load_image(image) \n",
    "                                         for image in batch_image_paths])\n",
    "                batch_x = [batch_images, batch_partial_sentances]\n",
    "                batch_y = np.zeros((batch_size, self.vocab_size))\n",
    "                \n",
    "                for i, next_word in enumerate(batch_next_words):\n",
    "                    batch_y[i][next_word] = 1.0\n",
    "                    \n",
    "                yield [batch_x, batch_y]\n",
    "                \n",
    "                \n",
    "partial_sentances, images, next_words = zip(*train_set)\n",
    "data_gen = DataGenerator(partial_sentances, \n",
    "                         images, \n",
    "                         next_words, \n",
    "                         unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "_This will take some time._ \n",
    "\n",
    "We define a checkpoint that saves the weights if the loss is improved. This is a form of early stopping. Go find something else to do for the day (or week depending on the quality of your graphics card) and leave this running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5057/5056 [==============================] - 2377s 470ms/step - loss: 1.2885 - acc: 0.8898\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.28846, saving model to weights-improvement-01.hdf5\n",
      "Epoch 2/50\n",
      " 230/5056 [>.............................] - ETA: 37:49 - loss: 1.4204 - acc: 0.8821"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "file_name = 'weights-improvement-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(file_name, \n",
    "                             monitor='loss',\n",
    "                             verbose=1, \n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "parallel_model.fit_generator(data_gen.data_generator(batch_size), \n",
    "                             steps_per_epoch=data_gen.data_size / batch_size, \n",
    "                             epochs=epochs, \n",
    "                             verbose=1, \n",
    "                             callbacks=callbacks_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
