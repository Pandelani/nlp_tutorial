{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='resources/dl.jpg'>\n",
    "\n",
    "# Sentiment Analysis with Keras\n",
    "\n",
    "### Keras\n",
    "\n",
    "Keras is a deep learning library that focuses on providing a high level neural network API that comes with the best practicises included. It's ideal for beginners until you need more advanced features or know why or why you wont need the various best practices.\n",
    "\n",
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\n",
      "E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot)\n"
     ]
    }
   ],
   "source": [
    "!apt-get install graphviz\n",
    "!pip install graphviz pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie reviews sentiment classification\n",
    "\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "\n",
    "For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\", by using the ```num_words``` argument.\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words_tokenised = 20000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=number_of_words_tokenised,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the dataset\n",
    "\n",
    "Here we go from sequences of embedding indices to sentances and back again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sentance_to_indices(sentance, \n",
    "                                max_sentance_length,\n",
    "                                word_to_index_dict, \n",
    "                                start_index=1, \n",
    "                                unknown_index=2,\n",
    "                                pad_index=0,\n",
    "                                remove_punctuation=False):\n",
    "    \n",
    "    if remove_punctuation:\n",
    "        strip_table = str.maketrans({key: None for key in string.punctuation})\n",
    "        sentance = sentance.translate(strip_table)\n",
    "        \n",
    "    indices = [start_index]\n",
    "    for word_count, word in enumerate(sentance.split()):\n",
    "        \n",
    "        if word_count >= max_sentance_length:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            index = word_to_index_dict[word]\n",
    "        except:\n",
    "            index = unknown_index\n",
    "        indices.append(index)\n",
    "        \n",
    "    while len(indices) < max_sentance_length:\n",
    "        indices = [pad_index] + indices\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def convert_indices_to_sentance(indices, index_to_word_dict):\n",
    "    words = [index_to_word_dict[index] for index in indices]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index_dict = imdb.get_word_index()\n",
    "word_to_index_dict = {key: (value + 3) for key, value in word_to_index_dict.items()}\n",
    "word_to_index_dict['<START>'] = 1\n",
    "word_to_index_dict['<PAD>'] = 0\n",
    "word_to_index_dict['<UNKNOWN>'] = 2\n",
    "\n",
    "index_to_word_dict = {value: key for key, value in word_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** What the neural network will see: **\n",
      "[    0     0     0     0     0     0     0     0     0     0     1    14\n",
      "    22     9  1329     8    30     6   147  7942   894    25   391     4\n",
      "  2331   467    63    12    16    93     4    71  2218     8    30   177\n",
      "    11     4    22   310     7     6   297    15    16    24  1822   398\n",
      "    18    98    36    26   432     7  1147 16756    83     4   555  3614\n",
      "    12   238    28    77  2805    48    12    69   343   275   156    37\n",
      "   122    24    28   141   312  1398  2611   725    98   305    12    47\n",
      "    77  1437     2    17   173     7     6 10070     8    63    12    66\n",
      "   152  1833     8  4881]\n",
      "\n",
      "** What we humans like to see: **\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> this film is likely to be a real letdown unless you understand the circumstances under which it was made the were chosen to be cast in the film version of a play that was not originally written for them they are sort of force fitted into the roles ironically it might have been funnier if it had used different actors who did not have such high expectations placed upon them instead it has been forever <UNKNOWN> as part of a canon to which it really doesn't deserve to belong\n",
      "\n",
      "** What we want the neural network to predict: **\n",
      "0\n",
      "\n",
      "** The label is: **\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "\n",
    "sentance = convert_indices_to_sentance(x_test[index], index_to_word_dict)\n",
    "label = y_test[index]\n",
    "\n",
    "print(\"\\n** What the neural network will see: **\")\n",
    "print(x_test[index])\n",
    "\n",
    "print(\"\\n** What we humans like to see: **\")\n",
    "print(sentance)\n",
    "\n",
    "print(\"\\n** What we want the neural network to predict: **\")\n",
    "print(label)\n",
    "\n",
    "print(\"\\n** The label is: **\")\n",
    "print(\"Positive\" if label == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making models is like playing with Lego\n",
    "\n",
    "\n",
    "Keras allows us to play with neural architecture lego blocks and stack things together in weird and wonderful ways. For now don't get bogged down in the theory of what works and doesn't work. Just be aware of what the shape each block needs as an input, and what the shape each block will output.\n",
    "\n",
    "<img src=\"resources/tower.jpg\" width=\"400\">\n",
    "\n",
    "Below are a bunch of models that you can try and extend, aswell as a blank function you could fill in with your own creation! [Feel free to browse the reference for available neural lego blocks to use.](https://keras.io/layers/about-keras-layers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn(number_of_words_tokenised, rnn_type='lstm', hidden_size=64, keep_prob=0.5):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(number_of_words_tokenised, 128))\n",
    "    \n",
    "    if rnn_type is 'lstm':\n",
    "        rnn_cell = LSTM(hidden_size, dropout=keep_prob, recurrent_dropout=keep_prob)\n",
    "    else:\n",
    "        rnn_cell = GRU(hidden_size, dropout=keep_prob, recurrent_dropout=keep_prob)\n",
    "        \n",
    "    model.add(rnn_cell)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def bidirectional_rnn(number_of_words_tokenised, rnn_type='lstm', hidden_size=64, keep_prob=0.5):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(number_of_words_tokenised, 128, input_length=maxlen))\n",
    "    \n",
    "    rnn_cell = LSTM(hidden_size) if rnn_type is 'lstm' else GRU(hidden_size)\n",
    "    \n",
    "    model.add(Bidirectional(rnn_cell))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def convolutional(number_of_words_tokenised, filters = 250, kernel_size = 3, hidden_dims=250, keep_prob=0.2):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(number_of_words_tokenised, 128, input_length=maxlen))\n",
    "    model.add(Dropout(keep_prob))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def my_model(number_of_words_tokenised):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # ... Insert code here ...\n",
    "    # Ensure that the model ends with a single\n",
    "    # sigmoidal unit.\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct desired model and view structure\n",
    "\n",
    "We choose the function we would like to construct our model and we then run it through a few helper functions to visualise the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"304pt\" viewBox=\"0.00 0.00 355.00 304.00\" width=\"355pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 351,-300 351,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140130953218760 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140130953218760</title>\n",
       "<polygon fill=\"none\" points=\"-2.84217e-14,-249.5 -2.84217e-14,-295.5 347,-295.5 347,-249.5 -2.84217e-14,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"100\" y=\"-268.8\">embedding_14_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"200,-249.5 200,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"200,-272.5 255,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"255,-249.5 255,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"255,-272.5 347,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140130953217808 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140130953217808</title>\n",
       "<polygon fill=\"none\" points=\"2.5,-166.5 2.5,-212.5 344.5,-212.5 344.5,-166.5 2.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"86.5\" y=\"-185.8\">embedding_14: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"170.5,-166.5 170.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"170.5,-189.5 225.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"198\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"225.5,-166.5 225.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"285\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"225.5,-189.5 344.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"285\" y=\"-174.3\">(None, None, 128)</text>\n",
       "</g>\n",
       "<!-- 140130953218760&#45;&gt;140130953217808 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140130953218760-&gt;140130953217808</title>\n",
       "<path d=\"M173.5,-249.366C173.5,-241.152 173.5,-231.658 173.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"177,-222.607 173.5,-212.607 170,-222.607 177,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140130953217304 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140130953217304</title>\n",
       "<polygon fill=\"none\" points=\"34,-83.5 34,-129.5 313,-129.5 313,-83.5 34,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"86.5\" y=\"-102.8\">lstm_11: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"139,-83.5 139,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"139,-106.5 194,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"194,-83.5 194,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.5\" y=\"-114.3\">(None, None, 128)</text>\n",
       "<polyline fill=\"none\" points=\"194,-106.5 313,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"253.5\" y=\"-91.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 140130953217808&#45;&gt;140130953217304 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140130953217808-&gt;140130953217304</title>\n",
       "<path d=\"M173.5,-166.366C173.5,-158.152 173.5,-148.658 173.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"177,-139.607 173.5,-129.607 170,-139.607 177,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140130788234408 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140130788234408</title>\n",
       "<polygon fill=\"none\" points=\"53.5,-0.5 53.5,-46.5 293.5,-46.5 293.5,-0.5 53.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"108\" y=\"-19.8\">dense_17: Dense</text>\n",
       "<polyline fill=\"none\" points=\"162.5,-0.5 162.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"162.5,-23.5 217.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"217.5,-0.5 217.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5\" y=\"-31.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"217.5,-23.5 293.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"255.5\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 140130953217304&#45;&gt;140130788234408 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140130953217304-&gt;140130788234408</title>\n",
       "<path d=\"M173.5,-83.3664C173.5,-75.1516 173.5,-65.6579 173.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"177,-56.6068 173.5,-46.6068 170,-56.6069 177,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnn(number_of_words_tokenised)\n",
    "# model = bidirectional_rnn(number_of_words_tokenised)\n",
    "# model = convolutional(number_of_words_tokenised)\n",
    "# model = my_model(number_of_words_tokenised)\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 96s - loss: 0.5163 - acc: 0.7416 - val_loss: 0.4198 - val_acc: 0.8076\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 91s - loss: 0.3822 - acc: 0.8361 - val_loss: 0.3935 - val_acc: 0.8288\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 91s - loss: 0.3097 - acc: 0.8726 - val_loss: 0.3957 - val_acc: 0.8306\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 96s - loss: 0.2570 - acc: 0.8988 - val_loss: 0.4216 - val_acc: 0.8363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72bc9edc18>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 4\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model!\n",
    "\n",
    "The first step is to create a bunch of sentances that are either positive or negative. The model will then predict this. Because the model ends with a single sigmoidal neuron, which means the value it will output will be squashed between zero and one, we can simply binarise the value by checking if it is greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 'good movie' was a positive sentance\n",
      "1. 'bad movie' was a negative sentance\n",
      "2. 'I hate big corporations, especially tech companies like facebook.' was a negative sentance\n",
      "3. 'I love being a contrarian.' was a positive sentance\n",
      "4. 'Argh! Fuck this!' was a negative sentance\n",
      "5. 'To be honest this was a fun thing.' was a positive sentance\n",
      "6. 'Just tried watching Modern Family - written by a moron, really boring. Writer has the mind of a very dumb and backward child.' was a negative sentance\n",
      "7. 'I look very much forward to showing my financials, because they are huge.' was a positive sentance\n"
     ]
    }
   ],
   "source": [
    "test_sentances = []\n",
    "\n",
    "test_sentances.append(\"good movie\")\n",
    "test_sentances.append(\"bad movie\")\n",
    "test_sentances.append(\"I hate big corporations, especially tech companies like facebook.\")\n",
    "test_sentances.append(\"I love being a contrarian.\")\n",
    "test_sentances.append(\"Argh! Fuck this!\")\n",
    "test_sentances.append(\"To be honest this was a fun thing.\")\n",
    "test_sentances.append(\"Just tried watching Modern Family - written by a moron, really boring. Writer has the mind of a very dumb and backward child.\")\n",
    "test_sentances.append(\"I look very much forward to showing my financials, because they are huge.\")\n",
    "\n",
    "for i, test_sentance in enumerate(test_sentances):\n",
    "    \n",
    "    test_sentance_indices = convert_sentance_to_indices(test_sentance, \n",
    "                                                        maxlen, \n",
    "                                                        word_to_index_dict)\n",
    "    test_sentance_indices = np.array(test_sentance_indices).reshape((1, -1))\n",
    "\n",
    "    prediction = model.predict(x=test_sentance_indices)[0]\n",
    "\n",
    "    if prediction < 0.5:\n",
    "        print(\"{}. '{}' was a negative sentance\".format(i, test_sentance))\n",
    "    else:\n",
    "        print(\"{}. '{}' was a positive sentance\".format(i, test_sentance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
