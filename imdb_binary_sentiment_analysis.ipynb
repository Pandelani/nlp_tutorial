{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='resources/dl.jpg'>\n",
    "\n",
    "# Sentiment Analysis with Keras\n",
    "\n",
    "### Keras\n",
    "\n",
    "Keras is a deep learning library that focuses on providing a high level neural network API that comes with the best practicises included. It's ideal for beginners until you need more advanced features or know why or why you wont need the various best practices.\n",
    "\n",
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\n",
      "E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.5/dist-packages (from pydot)\n"
     ]
    }
   ],
   "source": [
    "!apt-get install graphviz\n",
    "!pip install graphviz pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eavi/leon/keras_env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout, Embedding, LSTM, GRU, Bidirectional, Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Movie reviews sentiment classification\n",
    "\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "\n",
    "For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\", by using the ```num_words``` argument.\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "#### Special Tokens:\n",
    "\n",
    "```\n",
    "0: <PAD>\n",
    "1: <START>\n",
    "2: <UNKNOWN>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "number_of_words_tokenised = 20000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=number_of_words_tokenised,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train reviews\n",
      "25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train reviews')\n",
    "print(len(x_test), 'test reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 25000 arbitrarily lengthed sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), 218, 189)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, len(x_train[0]), len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 100\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the dataset\n",
    "\n",
    "Here we go from sequences of embedding indices to sentances and back again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentance_to_indices(sentance, \n",
    "                                max_sentance_length,\n",
    "                                word_to_index_dict, \n",
    "                                start_index=1, \n",
    "                                unknown_index=2,\n",
    "                                pad_index=0,\n",
    "                                remove_punctuation=False):\n",
    "    \n",
    "    if remove_punctuation:\n",
    "        strip_table = str.maketrans({key: None for key in string.punctuation})\n",
    "        sentance = sentance.translate(strip_table)\n",
    "        \n",
    "    indices = [start_index]\n",
    "    for word_count, word in enumerate(sentance.split()):\n",
    "        \n",
    "        if word_count >= max_sentance_length:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            index = word_to_index_dict[word]\n",
    "        except:\n",
    "            index = unknown_index\n",
    "        indices.append(index)\n",
    "        \n",
    "    while len(indices) < max_sentance_length:\n",
    "        indices = [pad_index] + indices\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "def convert_indices_to_sentance(indices, index_to_word_dict):\n",
    "    words = [index_to_word_dict[index] for index in indices]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "word_to_index_dict = imdb.get_word_index()\n",
    "word_to_index_dict = {key: (value + 3) for key, value in word_to_index_dict.items()}\n",
    "word_to_index_dict['<START>'] = 1\n",
    "word_to_index_dict['<PAD>'] = 0\n",
    "word_to_index_dict['<UNKNOWN>'] = 2\n",
    "\n",
    "index_to_word_dict = {value: key for key, value in word_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** What the neural network will see: **\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     1   591   202    14\n",
      "    31     6   717    10    10 18142 10698     5     4   360     7     4\n",
      "   177  5760   394   354     4   123     9  1035  1035  1035    10    10\n",
      "    13    92   124    89   488  7944   100    28  1668    14    31    23\n",
      "    27  7479    29   220   468     8   124    14   286   170     8   157\n",
      "    46     5    27   239    16   179 15387    38    32    25  7944   451\n",
      "   202    14     6   717]\n",
      "\n",
      "** What we humans like to see: **\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n",
      "\n",
      "** What we want the neural network to predict: **\n",
      "0\n",
      "\n",
      "** The label is: **\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "sentance = convert_indices_to_sentance(x_test[index], index_to_word_dict)\n",
    "label = y_test[index]\n",
    "\n",
    "print(\"\\n** What the neural network will see: **\")\n",
    "print(x_test[index])\n",
    "\n",
    "print(\"\\n** What we humans like to see: **\")\n",
    "print(sentance)\n",
    "\n",
    "print(\"\\n** What we want the neural network to predict: **\")\n",
    "print(label)\n",
    "\n",
    "print(\"\\n** The label is: **\")\n",
    "print(\"Positive\" if label == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making models is like playing with Lego\n",
    "\n",
    "\n",
    "Keras allows us to play with neural architecture lego blocks and stack things together in weird and wonderful ways. For now don't get bogged down in the theory of what works and doesn't work. Just be aware of what the shape each block needs as an input, and what the shape each block will output.\n",
    "\n",
    "<img src=\"resources/tower.jpg\" width=\"400\">\n",
    "\n",
    "Below are a bunch of models that you can try and extend, aswell as a blank function you could fill in with your own creation! [Feel free to browse the reference for available neural lego blocks to use.](https://keras.io/layers/about-keras-layers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(number_of_words_tokenised, sizes, keep_prob=0.1):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions.\n",
    "    model.add(Embedding(number_of_words_tokenised, 50, input_length=maxlen))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # For each size in the hidden sizes, add a vanilla hidden layer:\n",
    "    for size in sizes:\n",
    "        model.add(Dense(size))\n",
    "        model.add(Dropout(keep_prob))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def rnn(number_of_words_tokenised, rnn_type='lstm', hidden_size=64, keep_prob=0.5):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(number_of_words_tokenised, 128))\n",
    "    \n",
    "    if rnn_type is 'lstm':\n",
    "        rnn_cell = LSTM(hidden_size, dropout=keep_prob, recurrent_dropout=keep_prob)\n",
    "    else:\n",
    "        rnn_cell = GRU(hidden_size, dropout=keep_prob, recurrent_dropout=keep_prob)\n",
    "        \n",
    "    model.add(rnn_cell)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def bidirectional_rnn(number_of_words_tokenised, rnn_type='lstm', hidden_size=64, keep_prob=0.5):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(number_of_words_tokenised, 128, input_length=maxlen))\n",
    "    \n",
    "    rnn_cell = LSTM(hidden_size) if rnn_type is 'lstm' else GRU(hidden_size)\n",
    "    \n",
    "    model.add(Bidirectional(rnn_cell))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def convolutional(number_of_words_tokenised, filters=250, kernel_size=3, hidden_dims=250, keep_prob=0.2):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(number_of_words_tokenised, 128, input_length=maxlen))\n",
    "    model.add(Dropout(keep_prob))\n",
    "\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters,\n",
    "                     kernel_size,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(keep_prob))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def my_model(number_of_words_tokenised):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # ... Insert code here ...\n",
    "    # Ensure that the model ends with a single\n",
    "    # sigmoidal unit.\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct desired model and view structure\n",
    "\n",
    "We choose the function we would like to construct our model and we then run it through a few helper functions to visualise the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"304pt\" viewBox=\"0.00 0.00 348.00 304.00\" width=\"348pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 344,-300 344,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139812846925864 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139812846925864</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 340,-295.5 340,-249.5 0,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"96.5\" y=\"-268.8\">embedding_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"193,-249.5 193,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"193,-272.5 248,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"248,-249.5 248,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"248,-272.5 340,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"294\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 139812846925640 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139812846925640</title>\n",
       "<polygon fill=\"none\" points=\"2.5,-166.5 2.5,-212.5 337.5,-212.5 337.5,-166.5 2.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-185.8\">embedding_1: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"163.5,-166.5 163.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"163.5,-189.5 218.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"218.5,-166.5 218.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"218.5,-189.5 337.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278\" y=\"-174.3\">(None, None, 128)</text>\n",
       "</g>\n",
       "<!-- 139812846925864&#45;&gt;139812846925640 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139812846925864-&gt;139812846925640</title>\n",
       "<path d=\"M170,-249.366C170,-241.152 170,-231.658 170,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"173.5,-222.607 170,-212.607 166.5,-222.607 173.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139812845113072 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139812845113072</title>\n",
       "<polygon fill=\"none\" points=\"34,-83.5 34,-129.5 306,-129.5 306,-83.5 34,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"83\" y=\"-102.8\">lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"132,-83.5 132,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"132,-106.5 187,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187,-83.5 187,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.5\" y=\"-114.3\">(None, None, 128)</text>\n",
       "<polyline fill=\"none\" points=\"187,-106.5 306,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246.5\" y=\"-91.3\">(None, 64)</text>\n",
       "</g>\n",
       "<!-- 139812846925640&#45;&gt;139812845113072 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139812846925640-&gt;139812845113072</title>\n",
       "<path d=\"M170,-166.366C170,-158.152 170,-148.658 170,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"173.5,-139.607 170,-129.607 166.5,-139.607 173.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139812846925696 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139812846925696</title>\n",
       "<polygon fill=\"none\" points=\"53.5,-0.5 53.5,-46.5 286.5,-46.5 286.5,-0.5 53.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"104.5\" y=\"-19.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"155.5,-0.5 155.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"155.5,-23.5 210.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"210.5,-0.5 210.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-31.3\">(None, 64)</text>\n",
       "<polyline fill=\"none\" points=\"210.5,-23.5 286.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 139812845113072&#45;&gt;139812846925696 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139812845113072-&gt;139812846925696</title>\n",
       "<path d=\"M170,-83.3664C170,-75.1516 170,-65.6579 170,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"173.5,-56.6068 170,-46.6068 166.5,-56.6069 173.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = mlp(number_of_words_tokenised, sizes=[1000, 50, 10])\n",
    "model = rnn(number_of_words_tokenised)\n",
    "# model = bidirectional_rnn(number_of_words_tokenised)\n",
    "# model = convolutional(number_of_words_tokenised)\n",
    "# model = my_model(number_of_words_tokenised)\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Cross entropy is defined as:\n",
    "$$-{(y\\log(p) + (1 - y)\\log(1 - p))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/while/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/while/TensorArrayReadV3, lstm_1/while/mul_1/Enter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_115 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2456_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'lstm_1/while/mul_1', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-0ac6d69b0eec>\", line 2, in <module>\n    model = rnn(number_of_words_tokenised)\n  File \"<ipython-input-14-d9341a11e234>\", line 34, in rnn\n    model.add(rnn_cell)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 499, in __call__\n    return super(RNN, self).__call__(inputs, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 2063, in call\n    initial_state=initial_state)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 608, in call\n    input_length=timesteps)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2764, in rnn\n    swap_memory=True)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2750, in _step\n    tuple(constants))\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 599, in step\n    return self.cell.call(inputs, states, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 1824, in call\n    inputs_f = inputs * dp_mask[1]\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 907, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1131, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2798, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/while/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/while/TensorArrayReadV3, lstm_1/while/mul_1/Enter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_115 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2456_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/while/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/while/TensorArrayReadV3, lstm_1/while/mul_1/Enter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_115 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2456_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-907de5891786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           validation_data=[x_test, y_test])\n\u001b[0m",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/while/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/while/TensorArrayReadV3, lstm_1/while/mul_1/Enter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_115 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2456_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'lstm_1/while/mul_1', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-15-0ac6d69b0eec>\", line 2, in <module>\n    model = rnn(number_of_words_tokenised)\n  File \"<ipython-input-14-d9341a11e234>\", line 34, in rnn\n    model.add(rnn_cell)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/models.py\", line 492, in add\n    output_tensor = layer(self.outputs[0])\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 499, in __call__\n    return super(RNN, self).__call__(inputs, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 2063, in call\n    initial_state=initial_state)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 608, in call\n    input_length=timesteps)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2764, in rnn\n    swap_memory=True)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2750, in _step\n    tuple(constants))\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 599, in step\n    return self.cell.call(inputs, states, **kwargs)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/keras/layers/recurrent.py\", line 1824, in call\n    inputs_f = inputs * dp_mask[1]\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 907, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1131, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2798, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/eavi/leon/keras_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: lstm_1/while/mul_1 = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/while/TensorArrayReadV3, lstm_1/while/mul_1/Enter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_115 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2456_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 4\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model!\n",
    "\n",
    "The first step is to create a bunch of sentances that are either positive or negative. The model will then predict this. Because the model ends with a single sigmoidal neuron, which means the value it will output will be squashed between zero and one, we can simply binarise the value by checking if it is greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 'good movie' was a positive sentance\n",
      "1. 'bad movie' was a negative sentance\n",
      "2. 'I hate big corporations, especially tech companies like facebook.' was a negative sentance\n",
      "3. 'I love being a contrarian.' was a positive sentance\n",
      "4. 'Argh! Fuck this!' was a positive sentance\n",
      "5. 'To be honest this was a fun thing.' was a positive sentance\n",
      "6. 'Just tried watching Modern Family - written by a moron, really boring. Writer has the mind of a very dumb and backward child.' was a negative sentance\n",
      "7. 'I look very much forward to showing my financials, because they are huge.' was a positive sentance\n"
     ]
    }
   ],
   "source": [
    "test_sentances = []\n",
    "\n",
    "test_sentances.append(\"good movie\")\n",
    "test_sentances.append(\"bad movie\")\n",
    "test_sentances.append(\"I hate big corporations, especially tech companies like facebook.\")\n",
    "test_sentances.append(\"I love being a contrarian.\")\n",
    "test_sentances.append(\"Argh! Fuck this!\")\n",
    "test_sentances.append(\"To be honest this was a fun thing.\")\n",
    "test_sentances.append(\"Just tried watching Modern Family - written by a moron, really boring. Writer has the mind of a very dumb and backward child.\")\n",
    "test_sentances.append(\"I look very much forward to showing my financials, because they are huge.\")\n",
    "\n",
    "for i, test_sentance in enumerate(test_sentances):\n",
    "    \n",
    "    test_sentance_indices = convert_sentance_to_indices(test_sentance, \n",
    "                                                        maxlen, \n",
    "                                                        word_to_index_dict)\n",
    "    test_sentance_indices = np.array(test_sentance_indices).reshape((1, -1))\n",
    "\n",
    "    prediction = model.predict(x=test_sentance_indices)[0]\n",
    "\n",
    "    if prediction < 0.5:\n",
    "        print(\"{}. '{}' was a negative sentance\".format(i, test_sentance))\n",
    "    else:\n",
    "        print(\"{}. '{}' was a positive sentance\".format(i, test_sentance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
